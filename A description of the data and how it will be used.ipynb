{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": true
            },
            "source": "# A description of the data and how it will be used #\n\nWhen it comes to Data analysis there are many steps to take into consideration such as:\n\n1. **data collection**\n2. **data exploration** \n3. **data preparation**\n4. **data modelling**\n5. **data evaluation**\n\nThe source of data is the first step of the way. It is crucial to find proper sources of data for your analysis. The assignment will focus on data regarding car accidents in the city of Leeds, UK. The dataset was retrieved directly from the UK government\u2019s website: https://data.gov.uk/dataset/6efe5505-941f-45bf-b576-4c1e09b579a1/road-traffic-accidents/datafile/c885f604-ee28-418d-a988-160cde514756/preview\n\nThis dataset includes a set of information regarding car accidents in Leeds in the year 2019. Pretty good, since it is up-to-date.\n\nOnce imported all the necessary libraries and the data needed (in this case we will use a csv file imported directly in the notebook) we are ready to observe the data and, if necesary, clean the dataset from raw data and get it ready for analysis. One step more is to look for data types in your datasets.\n\nHere is the list of all the variables in the dataset including their datatype.\n\n1. **Grid Ref: Easting** --> int64\n2. **Grid Ref: Northing** --> int64\n3. **Number of Vehicles**  --> int64\n4. **Accident Date** --> object\n5. **Time (24hr)** --> int64\n6. **1st Road Class** --> int64\n7. **1st Road Class & No**  --> object\n8. **Road Surface** --> int64\n9. **Lighting Conditions** --> int64\n10. **Weather Conditions** --> int64\n11. **Local Authority** --> object\n12. **Vehicle Number**  --> int64\n13. **Type of Vehicle** --> int64\n14. **Casualty Class** --> int64\n15. **Casualty Severity** --> int64\n16. **Sex of Casualty** --> int64\n17. **Age of Casualty** --> int64\n\nNow this data will be manipulated, filtered and prepared in such a way that will favour the data modelling stage. The data will be used for the purpose of suggesting proper methods to mitigate the risk of car accidents based on machine learning algorithms built with the variables that most impact road safety and car accidents occurrance.\n\n\n#### Data Preparation ####\n\nData preparation is the process of cleaning and transforming raw data prior to processing and analysis. It is an important step prior to processing and often involves reformatting data, making corrections to data and the combining of data sets to enrich data (standardizing data formats, enriching source data, and/or removing outliers). In the dataset null values or missing values will be removed, existing data correlation will be analyzed and more manipulation on data will be performed.\n\n\n#### Data Modelling ####\nAt this stage we will implement machine learning algoritmhs to create and evaluate models.\nHere is the list of commonly used machine learning algorithms. These algorithms can be applied to almost any data problem:\n\n1. Linear Regression\n2. Logistic Regression\n3. Decision Tree\n4. SVM\n5. Naive Bayes\n6. kNN\n7. K-Means\n8. Random Forest\n9. Dimensionality Reduction Algorithms\n10. Gradient Boosting algorithms (GBM, XGBoost, LightGBM, CatBoost)\n\nSome of this algorithms will be tested and accuracy of each will be evaluated.\n\n#### Data Evaluation ####\nBefore proceeding to the deployment stage, the model needs to be evaluated thoroughly to ensure that the business or the applications' objectives are achieved. Certain metrics can be used for the model evaluation such as accuracy, recall, F1-score, precision, and others.\n\n#### Data Deployment ####\nThe concept of deployment in data science refers to the application of a model for prediction using a new data. Building a model is generally not the end of the project. Even if the purpose of the model is to increase knowledge of the data, the knowledge gained will need to be organized and presented. Depending on the requirements, the deployment phase can be as simple as generating a report or as complex as implementing a repeatable data science process.\n\n\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}